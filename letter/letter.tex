\documentclass[12pt]{article}
\usepackage{fullpage}
\begin{document}
Any any sports ranking is going to be contentious. Players, coaches, and teams are often judged subjectively, not so much by how many games they win so much as how inspiring their story is. When we were given the challenge of deciding the greatest college coaches of all time, we knew that we'd have to be as objective as possible. Rather than choose any one way of picking the greatest coaches, we explored several metrics and compared them to existing opinion polls and awards.

\section{Common Models}
We considered how well the most common metrics for rating coaches work in this situation. Win ratio (wins over total games) is often used. In our situation, we found that this doesn't work because it will put a coach with a 1-0 record above one with a 200-20 record. The reason why win ratio works when ranking good coaches is usually because every coach surveyed has played a similar number of games. We could have filtered our data (only consider coaches with a bowl win, etc.), but we wanted to remove arbitrary factors from our process. Another metric we tested was the win-loss difference. We found that this simply favored coaches with long careers.

One method we tested that produced great results was to multiply the win ratio by the number of wins. This effectively dismisses coaches who have played too few games, but doesn't allow one coach to dominate simply because of a high number of games.

\section{Connecting the Dots}
One problem with this model is that it doesn't consider the connections between coaches. Surely career stats don't matter when one coach consistently beats another. We explored the interactions between coaches with what is known as a graph model. We looked at what games we could, paired teams to coaches, and compiled statistics about the interactions between pairs of coaches.

For Basketball, we collected score data from every NCAA tournament game, and for Football we collected the results of every postseason bowl game. The original plan was to collect data on every regular season game ever as well, but that would have violated the data use policies of our sources. We were also unable to find any more data that tournament final scores for Baseball (the data exists but not in a usable format), and only considering two coaches every year would not have been enough data.

With the data that we could get, we did some math relating to the score differences and game importance. For instance, a first-round game is not as important as a Final Four game, and an easy victory weighs more heavily than a close win. We used this data to say whether one coach beats another over the course of their games together. With this, we used a variant of Google's Pagerank algorithm (originally designed to rank website results) to rank coaches. We call this system CoachRank.

The idea is like this: I get points from every coach that I beat, and I evenly distribute those points to coaches who beat me. We give every coach the same amount of points to start, and see where they stabilize. This adds transitivity to our model: if I beat another coach and he beats 100 other coaches, then I get a good share of all those points. This also has the effect of eliminating small outliers. A coach with a 15-0 record gets a lower score than one with a 200-30 record simply because the better coach has a better inflow of points.

\section{Results}
Based on our models, our top five coaches in football, basketball, and baseball are:
\begin{center}
\begin{tabular}{c | c | c | c}
Rank & Football & Basketball & Baseball\\\hline
1 & Joe Paterno  & Mike Krzyzewski & Ed Cheff \\
2 & Mack Brown   & Dean Smith      & Gene Stephenson\\
3 & Bear Bryant  & Roy Williams    & Mike Martin\\
4 & Lloyd Carr   & John Wooden     & Augie Garrido\\
5 & Pete Carroll & Rick Pitino     & Gordie Gillespie\\
\end{tabular}
\end{center}

\section{Commentary}
Akilesh's ML shit

\end{document}

